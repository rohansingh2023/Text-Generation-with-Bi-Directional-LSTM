{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adeba179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01dd5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def read_dataset(self,file):\n",
    "        letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "                   'n','o','p','q','r','s','t','u','v','w','x','y','z',' ']\n",
    "        # Open raw file\n",
    "        with open(file, 'r') as f:\n",
    "            raw_text = f.readlines()\n",
    "\n",
    "        # Convert into lowercase\n",
    "        raw_text = [line.lower() for line in raw_text]\n",
    "\n",
    "        # Create a string which contains the entire text\n",
    "        text_string = ''\n",
    "        for line in raw_text:\n",
    "            text_string += line.strip()\n",
    "\n",
    "        # Create an array by char\n",
    "        text = list()\n",
    "        for char in text_string:\n",
    "            text.append(char)\n",
    "\n",
    "        # Remove all symbosl and just keep letters\n",
    "        text = [char for char in text if char in letters]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def create_dictionary(self,text):\n",
    "        char_to_idx = dict()\n",
    "        idx_to_char = dict()\n",
    "\n",
    "        idx = 0\n",
    "        for char in text:\n",
    "            if char not in char_to_idx.keys():\n",
    "\n",
    "                # Build dictionaries\n",
    "                char_to_idx[char] = idx\n",
    "                idx_to_char[idx] = char\n",
    "                idx += 1\n",
    "        return char_to_idx, idx_to_char\n",
    "    \n",
    "    def build_sequences(self,text, char_to_idx, window):\n",
    "        x = list()\n",
    "        y = list()\n",
    "\n",
    "        for i in range(len(text)):\n",
    "            try:\n",
    "                # Get window of chars from text\n",
    "                # Then, transform it into its idx representation\n",
    "                sequence = text[i:i+window]\n",
    "                sequence = [char_to_idx[char] for char in sequence]\n",
    "\n",
    "                # Get word target\n",
    "                # Then, transfrom it into its idx representation\n",
    "                target = text[i+window]\n",
    "                target = char_to_idx[target]\n",
    "\n",
    "                # Save sequences and targets\n",
    "                x.append(sequence)\n",
    "                y.append(target)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01e80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneratorModel(nn.ModuleList):\n",
    "    def __init__(self,args,vocab_size):\n",
    "        super(TextGeneratorModel, self).__init__()\n",
    "        \n",
    "        self.batch_size = args.batch_size\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.input_size = vocab_size\n",
    "        self.num_layers = vocab_size\n",
    "        self.sequence_len = args.window\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
    "        \n",
    "        # Bi-LSTM\n",
    "        # Forward and backward\n",
    "        self.lstm_foward = nn.LSTMCell(self.hidden_dim, self.hidden_dim)        \n",
    "        self.lstm_backward = nn.LSTMCell(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTMCell(self.hidden_dim*2, self.hidden_dim*2)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(self.hidden_dim*2, self.num_layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Bi-LSTM\n",
    "        # hs = [batch_size x hidden_size]\n",
    "        # cs = [batch_size x hidden_size]\n",
    "        hs_forward = torch.zeros(x.size(0), self.hidden_dim)\n",
    "        cs_forward = torch.zeros(x.size(0), self.hidden_dim)\n",
    "        hs_backward = torch.zeros(x.size(0), self.hidden_dim)\n",
    "        cs_backward = torch.zeros(x.size(0), self.hidden_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        # hs = [batch_size x (hidden_size * 2)]\n",
    "        # cs = [batch_size x (hidden_size * 2)]\n",
    "        hs_lstm = torch.zeros(x.size(0), self.hidden_dim * 2)\n",
    "        cs_lstm = torch.zeros(x.size(0), self.hidden_dim * 2)\n",
    "        \n",
    "        # Weights initialization\n",
    "        nn.init.kaiming_normal_(hs_forward)\n",
    "        nn.init.kaiming_normal_(cs_forward)\n",
    "        nn.init.kaiming_normal_(hs_backward)\n",
    "        nn.init.kaiming_normal_(cs_backward)\n",
    "        nn.init.kaiming_normal_(hs_lstm)\n",
    "        nn.init.kaiming_normal_(cs_lstm)\n",
    "        \n",
    "        # From idx to embedding\n",
    "        out = self.embedding(x)\n",
    "        \n",
    "        # Prepare the shape for LSTM Cells\n",
    "        out = out.view(self.sequence_len, x.size(0), -1)\n",
    "        \n",
    "        forward = []\n",
    "        backward = []\n",
    "        \n",
    "        # Unfolding Bi-LSTM\n",
    "        # Forward\n",
    "        for i in range(self.sequence_len):\n",
    "            hs_forward, cs_forward = self.lstm_foward(out[i], (hs_backward,cs_forward))\n",
    "            hs_forward = self.dropout(hs_forward)\n",
    "            cs_forward = self.dropout(cs_forward)\n",
    "            forward.append(hs_forward)\n",
    "            \n",
    "        # Backward\n",
    "        for i in reversed(range(self.sequence_len)):\n",
    "            hs_backward, cs_backward = self.lstm_backward(out[i], (hs_backward, cs_backward))\n",
    "            hs_backward = self.dropout(hs_backward)\n",
    "            cs_backward = self.dropout(cs_backward)\n",
    "            backward.append(hs_backward)\n",
    "            \n",
    "        # LSTM\n",
    "        for fwd, bwd in zip(forward, backward):\n",
    "            input_tensor = torch.cat((fwd,bwd), 1)\n",
    "            hs_lstm , cs_lstm = self.lstm(input_tensor, (hs_lstm, cs_lstm))\n",
    "            \n",
    "        # Last hidden state is passed through a linear layer\n",
    "        out = self.linear(hs_lstm)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9fa5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Execution:\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.file = 'data/test.txt'\n",
    "        self.window = args.window\n",
    "        self.batch_size = args.batch_size\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.num_epochs = args.num_epochs\n",
    "\n",
    "        self.targets = None\n",
    "        self.sequences = None\n",
    "        self.vocab_size = None\n",
    "        self.char_to_idx = None\n",
    "        self.idx_to_char = None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "\n",
    "        # Initialize preprocessor object\n",
    "        preprocessing = Preprocessing()\n",
    "\n",
    "        # The 'file' is loaded and split by char\n",
    "        text = preprocessing.read_dataset(self.file)\n",
    "\n",
    "        # Given 'text', it is created two dictionaries\n",
    "        # a dictiornary about: from char to index\n",
    "                # a dictorionary about: from index to char\n",
    "        self.char_to_idx, self.idx_to_char = preprocessing.create_dictionary(text)\n",
    "\n",
    "        # Given the 'window', it is created the set of training sentences as well as\n",
    "        # the set of target chars\n",
    "        self.sequences, self.targets = preprocessing.build_sequences(text, self.char_to_idx, window=self.window)\n",
    "\n",
    "        # Gets the vocabuly size\n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "        \n",
    "    \n",
    "    def train(self, args):\n",
    "        \n",
    "        # Model initialization\n",
    "        model = TextGeneratorModel(args, self.vocab_size)\n",
    "        \n",
    "        # Optimizer initialization\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr= self.learning_rate)\n",
    "        \n",
    "        # Defining number of batches\n",
    "        num_batches = int(len(self.sequences)/self.batch_size)\n",
    "        \n",
    "        # Set model in training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Training phase\n",
    "        for epoch in range(self.num_epochs):\n",
    "            \n",
    "            # Mini batches\n",
    "            for i in range(num_batches):\n",
    "                \n",
    "                # Batch definition\n",
    "                try:\n",
    "                    x_batch = self.sequences[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                    y_batch = self.targets[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                except:\n",
    "                    x_batch = self.sequences[i * self.batch_size :]\n",
    "                    y_batch = self.targets[i * self.batch_size :]\n",
    "                    \n",
    "                # Convert numpy array into torch tensors\n",
    "                x = torch.from_numpy(x_batch).type(torch.LongTensor)\n",
    "                y = torch.from_numpy(y_batch).type(torch.LongTensor)\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = model(x)\n",
    "                \n",
    "                # Loss calc\n",
    "                loss = nn.functional.cross_entropy(y_pred, y.squeeze())\n",
    "                \n",
    "                # Clean gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update parameters through gradient descent\n",
    "                optimizer.step()\n",
    "                \n",
    "            print(\"Epoch: %d ,  loss: %.5f \" % (epoch, loss.item()))\n",
    "                \n",
    "        # Save weights\n",
    "#         torch.save(model.state_dict(), 'weights/textGenerator_model.pt')\n",
    "                \n",
    "    def generator(self,model, sequences, idx_to_char, n_char):\n",
    "        \n",
    "        # Set model in evaluation model\n",
    "        model.eval()\n",
    "        \n",
    "        # Softmax activation function\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Randomly is selected the index from the set of sequences\n",
    "        start = np.random.randint(0, len(sequences)-1)\n",
    "        \n",
    "        # The pattern is defined given the random idx\n",
    "        pattern = sequences[start]\n",
    "        \n",
    "        # By making use of the dictionaries, it is printed the pattern\n",
    "        print(\"\\nPattern: \\n\")\n",
    "        print(''.join([idx_to_char[value] for value in pattern]), \"\\\"\")\n",
    "        \n",
    "        # In full_prediction we will save the complete prediction\n",
    "        full_prediction = pattern.copy()\n",
    "        \n",
    "        # The prediction starts, it is going to be predicted a given\n",
    "        # number of characters\n",
    "        for i in range(n_char):\n",
    "            \n",
    "            # The numpy patterns is transformed into a tesor-type and reshaped\n",
    "            pattern = torch.from_numpy(pattern).type(torch.LongTensor)\n",
    "            pattern = pattern.view(1,-1)\n",
    "\n",
    "            # Make a prediction given the pattern\n",
    "            prediction = model(pattern)\n",
    "            # It is applied the softmax function to the predicted tensor\n",
    "            prediction = softmax(prediction)\n",
    "\n",
    "            # The prediction tensor is transformed into a numpy array\n",
    "            prediction = prediction.squeeze().detach().numpy()\n",
    "            # It is taken the idx with the highest probability\n",
    "            arg_max = np.argmax(prediction)\n",
    "\n",
    "            # The current pattern tensor is transformed into numpy array\n",
    "            pattern = pattern.squeeze().detach().numpy()\n",
    "            # The window is sliced 1 character to the right\n",
    "            pattern = pattern[1:]\n",
    "            # The new pattern is composed by the \"old\" pattern + the predicted character\n",
    "            pattern = np.append(pattern, arg_max)\n",
    "\n",
    "            # The full prediction is saved\n",
    "            full_prediction = np.append(full_prediction, arg_max)\n",
    "            \n",
    "        print(\"Prediction: \\n\")\n",
    "        print(' '.join([idx_to_char[value] for value in full_prediction]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76c8c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    window = 5\n",
    "    num_epochs = 50\n",
    "    hidden_dim = 128\n",
    "    batch_size = 6\n",
    "    learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13c79868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 ,  loss: 2.96876 \n",
      "Epoch: 1 ,  loss: 2.54420 \n",
      "Epoch: 2 ,  loss: 1.94019 \n",
      "Epoch: 3 ,  loss: 1.43155 \n",
      "Epoch: 4 ,  loss: 1.00472 \n",
      "Epoch: 5 ,  loss: 0.38241 \n",
      "Epoch: 6 ,  loss: 0.29214 \n",
      "Epoch: 7 ,  loss: 0.81126 \n",
      "Epoch: 8 ,  loss: 0.96055 \n",
      "Epoch: 9 ,  loss: 0.28479 \n",
      "Epoch: 10 ,  loss: 0.12264 \n",
      "Epoch: 11 ,  loss: 0.07859 \n",
      "Epoch: 12 ,  loss: 0.05342 \n",
      "Epoch: 13 ,  loss: 0.05423 \n",
      "Epoch: 14 ,  loss: 0.04125 \n",
      "Epoch: 15 ,  loss: 0.03350 \n",
      "Epoch: 16 ,  loss: 0.06954 \n",
      "Epoch: 17 ,  loss: 0.03093 \n",
      "Epoch: 18 ,  loss: 0.02811 \n",
      "Epoch: 19 ,  loss: 0.02445 \n",
      "Epoch: 20 ,  loss: 0.02176 \n",
      "Epoch: 21 ,  loss: 0.01909 \n",
      "Epoch: 22 ,  loss: 0.01707 \n",
      "Epoch: 23 ,  loss: 0.01412 \n",
      "Epoch: 24 ,  loss: 0.01264 \n",
      "Epoch: 25 ,  loss: 0.01401 \n",
      "Epoch: 26 ,  loss: 0.01140 \n",
      "Epoch: 27 ,  loss: 0.00915 \n",
      "Epoch: 28 ,  loss: 0.01103 \n",
      "Epoch: 29 ,  loss: 0.00881 \n",
      "Epoch: 30 ,  loss: 0.00922 \n",
      "Epoch: 31 ,  loss: 0.00760 \n",
      "Epoch: 32 ,  loss: 0.00602 \n",
      "Epoch: 33 ,  loss: 0.00572 \n",
      "Epoch: 34 ,  loss: 0.00687 \n",
      "Epoch: 35 ,  loss: 0.00462 \n",
      "Epoch: 36 ,  loss: 0.00537 \n",
      "Epoch: 37 ,  loss: 0.00559 \n",
      "Epoch: 38 ,  loss: 0.00447 \n",
      "Epoch: 39 ,  loss: 0.00737 \n",
      "Epoch: 40 ,  loss: 0.00482 \n",
      "Epoch: 41 ,  loss: 0.00441 \n",
      "Epoch: 42 ,  loss: 0.00349 \n",
      "Epoch: 43 ,  loss: 0.00392 \n",
      "Epoch: 44 ,  loss: 0.00312 \n",
      "Epoch: 45 ,  loss: 0.00264 \n",
      "Epoch: 46 ,  loss: 0.00217 \n",
      "Epoch: 47 ,  loss: 0.00268 \n",
      "Epoch: 48 ,  loss: 0.00245 \n",
      "Epoch: 49 ,  loss: 0.00280 \n"
     ]
    }
   ],
   "source": [
    "## Execution\n",
    "\n",
    "# Load and preprare the sequences\n",
    "execution = Execution(args)\n",
    "execution.prepare_data()\n",
    "\n",
    "# Training the model\n",
    "execution.train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7856a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp = Preprocessing()\n",
    "# pp.read_dataset('data/pg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b80613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing\n",
    "file = './data/test.txt'\n",
    "pp = Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3184d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pp.read_dataset(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30e12c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " ' ',\n",
       " 'd',\n",
       " 'o',\n",
       " 'c',\n",
       " 'u',\n",
       " 'm',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'o',\n",
       " 'p',\n",
       " 'e',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'e',\n",
       " 'd',\n",
       " 'u',\n",
       " 'c',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'r',\n",
       " 'p',\n",
       " 'o',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'n',\n",
       " 'l',\n",
       " 'y']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e87645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da1020f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[len(text)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b998ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc2f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pp.create_dictionary(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e2a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0196281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = pp.build_sequences(text, dic[0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d80b508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "028858f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d4d5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequen = \"This is a test.\"\n",
    "ox = pp.create_dictionary(sequen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "309a5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGeneratorModel(args,len(sequen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0588da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution.generator(model, sequen, ox[1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5bbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
